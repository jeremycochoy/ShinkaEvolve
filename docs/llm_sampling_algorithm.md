# LLM Dynamic Sampling Algorithm

## Overview

ShinkaEvolve uses an adaptive LLM selection strategy based on the **Asymmetric UCB1 (Upper Confidence Bound)** algorithm to dynamically choose which language model to use for code generation. This algorithm learns from the performance of programs generated by each LLM and adaptively samples models that produce better results.

**Key Principle**: The sampling is **performance-based**, not cost-based. The algorithm prioritizes LLMs that generate high-performing programs, regardless of their API pricing.

## Algorithm Components

### 1. Multi-Armed Bandit Framework

The LLM selection problem is modeled as a **multi-armed bandit**:
- Each **arm** represents an LLM model (e.g., GPT-4, Claude, Gemini)
- **Pulling an arm** means using that LLM to generate code
- **Reward** is the performance improvement achieved by the generated program

**Implementation**: `shinka/llm/dynamic_sampling.py:135-559` (`AsymmetricUCB` class)

### 2. Reward Signal: Program Performance

The reward signal is based on the `combined_score` of the generated program:

```python
# File: shinka/core/runner.py:883-913

reward = db_program.combined_score if correct_val else None
baseline = parent.combined_score if parent else None

result = self.llm_selection.update(
    arm=model_name,
    reward=reward,      # Performance score (if correct)
    baseline=baseline,  # Parent's performance score
)
```

**Reward Components**:
- **`reward`**: The `combined_score` of the newly generated program (if it passes validation), otherwise `None`
- **`baseline`**: The parent program's `combined_score` used for normalization
- **Normalized reward**: `r = reward - baseline` (improvement over parent)

**Key Insight**: Only programs that are **correct** (pass validation) provide a reward. Incorrect programs contribute `None`, which is imputed as the worst possible reward.

### 3. UCB1 Selection Strategy

The algorithm balances **exploitation** (using known good models) and **exploration** (trying undersampled models):

```python
# File: shinka/llm/dynamic_sampling.py:329-364

t = float(self.n.sum())                      # Total pulls across all arms
base = self._normalized_means(idx)            # Exploitation term
bonus = self.c * sqrt(2 * log(t) / n_sub)    # Exploration term
ucb_score = base + bonus                      # UCB score
```

**Terms**:
- **Exploitation (`base`)**: Mean normalized reward for each LLM
  - Higher for models that consistently improve over parent programs
- **Exploration (`bonus`)**: Uncertainty bonus
  - Higher for models that haven't been tried much
  - Decreases as more samples are collected
- **`c`** (exploration coefficient): Controls exploration vs exploitation trade-off (default: 1.0)

**Selection Process**:
1. Calculate UCB score for each model
2. With probability `1 - epsilon` (default 0.8): Select model with highest UCB score
3. With probability `epsilon` (default 0.2): Select random model (epsilon-greedy exploration)

### 4. Asymmetric Scaling

A key feature of the algorithm is **asymmetric reward scaling**:

```python
# File: shinka/llm/dynamic_sampling.py:222-278

if r > 0:  # Positive improvement
    scaled_r = r  # Full credit for improvement
else:      # No improvement or regression
    scaled_r = 0  # Treated as neutral (no penalty)
```

**Rationale**:
- Models are rewarded for **positive improvements** over the parent
- No explicit penalty for failing to improve (already captured by not getting positive reward)
- Encourages models that consistently make progress

### 5. Auto-Decay Mechanism

Past observations are exponentially decayed over time:

```python
# File: shinka/llm/dynamic_sampling.py:428-465

if self.auto_decay > 0:
    for arm in range(len(self.arms)):
        self.n[arm] *= self.auto_decay
        self.sum_r[arm] *= self.auto_decay
        self.sum_r2[arm] *= self.auto_decay
```

**Purpose**:
- **`auto_decay`** (default: 0.95): Exponential decay factor
- Recent performance is weighted more heavily than older results
- Allows the algorithm to adapt to:
  - Model improvements (e.g., new API versions)
  - Changing problem distributions over evolution stages
  - Prompt engineering updates

**Effect**: With decay factor 0.95, observations lose ~40% of their weight after ~10 updates.

## Configuration

### Enabling Dynamic Selection

In your evolution config file:

```yaml
# File: configs/evolution/medium_budget.yaml

llm_models:
  - "gemini-2.5-pro"
  - "gemini-2.5-flash"
  - "gpt-4.1-mini"
  - "gpt-4.1-nano"
  - "bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0"
  - "o4-mini"

llm_dynamic_selection: ucb  # Enable UCB-based sampling
```

**Options for `llm_dynamic_selection`**:
- `null` or omitted: Fixed equal probabilities for all models
- `"ucb"` or `"ucb1"`: Enable Asymmetric UCB1 algorithm
- Custom `BanditBase` instance: Use your own bandit algorithm

### UCB Parameters

**File**: `shinka/llm/dynamic_sampling.py:135-185`

```python
AsymmetricUCB(
    arms,                      # List of LLM model names
    exploration_coef=1.0,      # c: Exploration bonus coefficient
    epsilon=0.2,               # Epsilon-greedy exploration rate
    auto_decay=0.95,           # Exponential decay for past observations
    shift_by_baseline=True,    # Normalize by global baseline
    shift_by_parent=True,      # Normalize by parent score
    adaptive_scale=True,       # Use adaptive reward scaling
    asymmetric_scaling=True,   # Only reward positive improvements
    exponential_base=1.0,      # Optional exponential scaling
)
```

**Key Parameters**:
- **`exploration_coef`** (c): Higher values → more exploration
  - Range: 0.1 to 10.0
  - Default: 1.0 (standard UCB1)
- **`epsilon`**: Probability of random exploration
  - Range: 0.0 to 1.0
  - Default: 0.2 (20% random, 80% UCB-guided)
- **`auto_decay`**: Decay factor for past observations
  - Range: 0.0 to 1.0
  - Default: 0.95 (5% decay per update)
  - Set to 0.0 to disable decay
- **`asymmetric_scaling`**: Whether to use asymmetric reward scaling
  - Default: `True` (only reward improvements)

## How It Works in Practice

### Evolution Loop Integration

**File**: `shinka/core/runner.py:883-913`

```python
# After evaluating a generated program:

# 1. Get parent program (if exists)
parent = self.db.get(db_program.parent_id) if db_program.parent_id else None
baseline = parent.combined_score if parent else None

# 2. Calculate reward (program performance if correct)
reward = db_program.combined_score if correct_val else None

# 3. Update bandit algorithm with feedback
model_name = db_program.metadata["model_name"]
result = self.llm_selection.update(
    arm=model_name,
    reward=reward,
    baseline=baseline,
)
```

### Model Selection Flow

**File**: `shinka/llm/client.py:207-220`

```python
def get_kwargs(self):
    # Get sampling probabilities from bandit algorithm
    posterior = self.llm_selection.posterior()

    if self.verbose:
        logger.info(f"==> SAMPLING:")
        for name, prob in zip(self.model_names, posterior):
            logger.info(f"  {name:<30} {prob:>8.4f}")

    # Sample model based on probabilities
    return sample_model_kwargs(
        model_names=self.model_names,
        model_sample_probs=posterior,
        ...
    )
```

### Example Evolution Scenario

**Initial State** (no data):
- All models have equal probability: ~16.7% each (6 models)
- Epsilon-greedy ensures all models are tried

**After 100 Evaluations**:
```
Model                          Probability
gemini-2.5-pro                     0.3500  (35%)
claude-sonnet-4                    0.2800  (28%)
gpt-4.1-mini                       0.2000  (20%)
gemini-2.5-flash                   0.1200  (12%)
gpt-4.1-nano                       0.0300  (3%)
o4-mini                            0.0200  (2%)
```

Models that consistently generate programs with better `combined_score` get sampled more frequently.

## Performance Metrics

### What is `combined_score`?

The `combined_score` is the primary performance metric used for rewards. It typically combines:
- **Accuracy**: How well the program solves the task
- **Efficiency**: Runtime performance, memory usage
- **Simplicity**: Code complexity metrics

The exact definition depends on your evaluation configuration.

### Reward Calculation

```python
# Example calculation:

parent_score = 0.75       # Parent program performance
child_score = 0.85        # Generated program performance

reward_raw = child_score  # 0.85
baseline = parent_score   # 0.75
normalized_reward = reward_raw - baseline  # 0.85 - 0.75 = 0.10

# If asymmetric_scaling=True:
if normalized_reward > 0:
    final_reward = 0.10   # Positive improvement → gets credit
else:
    final_reward = 0.0    # No improvement → neutral
```

## Why Not Pricing-Based?

While LLM pricing data is available in `shinka/llm/models/pricing.py`, it is **only used for cost tracking**, not for selection.

**Rationale**:
- **Goal**: Find the best solution, not the cheapest
- **Cost optimization**: Handled by budget constraints and early stopping
- **Performance first**: Let the algorithm discover which models work best for the task
- **Natural efficiency**: If cheap models perform well, they'll be selected more often

**Cost tracking** is still available:
- Total API costs are logged
- Per-model costs are tracked
- Budget limits can be configured separately

## Advanced Topics

### Custom Bandit Algorithms

You can implement custom selection strategies by subclassing `BanditBase`:

```python
# File: shinka/llm/dynamic_sampling.py:16-133

class BanditBase(ABC):
    @abstractmethod
    def posterior(self, idx=None, k=1):
        """Return selection probabilities for each arm."""
        pass

    @abstractmethod
    def update(self, arm, reward, baseline=None):
        """Update algorithm with feedback."""
        pass

    @abstractmethod
    def update_submitted(self, arm):
        """Called when arm is selected; increments n_submitted to count model as 'tried' before evaluation completes."""
        pass
```

### Adaptive Scaling

When `adaptive_scale=True` (default), the algorithm uses:

```python
# File: shinka/llm/dynamic_sampling.py:222-278

# Compute running statistics for each arm
mean_r = self.sum_r[arm] / self.n[arm]
var_r = (self.sum_r2[arm] / self.n[arm]) - mean_r**2
std_r = sqrt(max(var_r, 1e-8))

# Scale rewards by standard deviation
scaled_reward = normalized_reward / (std_r + 1e-8)
```

**Purpose**: Normalize rewards to account for different reward distributions across models.

### Baseline Normalization

Two levels of baseline normalization:

1. **Parent baseline** (`shift_by_parent=True`):
   - Reward = `child_score - parent_score`
   - Measures improvement relative to parent

2. **Global baseline** (`shift_by_baseline=True`):
   - Reward = `(child_score - parent_score) - global_mean_improvement`
   - Measures improvement relative to average across all models

## Debugging and Monitoring

### Viewing Selection Probabilities

Enable verbose mode to see sampling probabilities:

```python
# In your config or code:
verbose = True
```

Output:
```
==> SAMPLING:
  gemini-2.5-pro                  0.3500
  claude-sonnet-4                 0.2800
  gpt-4.1-mini                    0.2000
  gemini-2.5-flash                0.1200
  gpt-4.1-nano                    0.0300
  o4-mini                         0.0200
```

### Analyzing Results

Check the evolution database for per-model performance:

```python
import sqlite3

conn = sqlite3.connect('evolution_results.db')
cursor = conn.cursor()

# Get average score by model
cursor.execute("""
    SELECT
        json_extract(metadata, '$.model_name') as model,
        AVG(combined_score) as avg_score,
        COUNT(*) as n_samples
    FROM programs
    WHERE is_correct = 1
    GROUP BY model
    ORDER BY avg_score DESC
""")

for row in cursor.fetchall():
    print(f"{row[0]:<30} {row[1]:.4f} ({row[2]} samples)")
```

## References

### Key Files

- **Bandit Algorithm**: `shinka/llm/dynamic_sampling.py`
  - `BanditBase` (lines 16-133): Abstract base class
  - `AsymmetricUCB` (lines 135-559): UCB1 implementation

- **Evolution Integration**: `shinka/core/runner.py`
  - Configuration (line 50): `llm_dynamic_selection` parameter
  - Feedback loop (lines 883-913): Reward updates

- **Model Selection**: `shinka/llm/client.py`
  - Sampling logic (lines 207-220): `get_kwargs()` method

- **Configuration Examples**: `configs/evolution/`
  - `medium_budget.yaml`: Standard UCB configuration
  - `large_budget.yaml`: Large-scale experiments

### Related Concepts

- **Multi-Armed Bandits**: [Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit)
- **UCB1 Algorithm**: Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). "Finite-time Analysis of the Multiarmed Bandit Problem"
- **Thompson Sampling**: Alternative bandit algorithm (not currently implemented)

## FAQ

**Q: Can I disable dynamic selection?**

A: Yes, omit `llm_dynamic_selection` or set it to `null` in your config. All models will have equal probability.

**Q: How many samples before the algorithm converges?**

A: Typically 50-100 evaluations per model. With 6 models, expect reasonable convergence after ~300 total evaluations.

**Q: Does the algorithm account for model latency?**

A: No, only performance (combined_score) is used. You could extend the reward function to include latency if desired.

**Q: What if I add a new model mid-evolution?**

A: The new model will start with no data and will be explored heavily initially (high exploration bonus).

**Q: How does this interact with temperature/sampling parameters?**

A: The bandit algorithm selects *which* model to use. Temperature controls randomness *within* that model's generation.
